Introduction to
Algorithms and
Complexity
Analysis
Ermira Daka
Agenda e Lendes
• 4x6= 24 ore ligjerata
• Hyrje ne Algoritme
• Algoritmet Divide and Conquer
• Algoritmet Dynamic Programming
• Algoritmet Greedy
• Grafet
• NP Completness
• Algorimet e perafrimit
• Algoritmet e Rendomta
Projekt
here
Key Aspects a Good Project Should
Cover:
Algorithm complexity analysis (time and space)
Implementation of known algorithms and possibly
designing new ones
Trade-offs between different algorithmic approaches
Real-world application of algorithms
Optimization techniques
Problem-solving under constraints
Assign yourself here latest tomorrow!
What is an
Algorithm?
• Definition: A step-by-step procedure to
solve a problem.
• Characteristics:
• Input: Takes input values.
• Output: Produces a result.
• Definiteness: Clearly defined steps.
• Finiteness: Terminates after a finite
number of steps.
• Effectiveness: Each step is simple
and can be executed.
• Examples: Recipe, Google search, ATM
transactions.
Algorithm
• Given a sorted array and a target value,
determine whether the target exists in the
array. If it does, return its index; otherwise,
return -1.
• Take some time and think the best way to
solve it…
Importance of
Algorithms
• Efficiency: Solves problems quickly.
• Scalability: Handles large inputs effectively.
• Optimization: Reduces computational cost.
• Applicability: Used in AI, data science, networking, security.
• Examples: Google PageRank, AI algorithms in gaming, GPS navigation.
Algorithm Design
Techniques
• Brute Force: Try all possibilities (e.g., Bubble Sort).
• Divide and Conquer: Break down problems (e.g., Merge Sort).
• Dynamic Programming: Solve subproblems once (e.g., Fibonacci).
• Greedy Approach: Make local optimal choices (e.g., Huffman
Coding).
• Backtracking & Branch and Bound: Systematic trial and error (e.g.,
N-Queens problem).
Introduction to
Complexity
Analysis
• Measures efficiency of an algorithm in terms of:
• Time Complexity: Number of operations performed.
• Space Complexity: Amount of memory used.
• Why analyze complexity?
• Helps compare algorithms.
• Predicts performance on large inputs.
• Guides optimization.
String (int x, int y) {
 int res=0;
 if(y<x){
 res=x-y;
 } else {
 res= y-x;
 }
 return ”Resultati=”+res;
}
Asymptotic
Notations
• Describe algorithm efficiency for large input sizes (n → ∞)
• Big-O (O): Upper bound (worst-case scenario).
• Big-Theta (Θ): Tight bound (average-case scenario).
• Big-Omega (Ω): Lower bound (best-case scenario).
• Examples:
• Linear Search: O(n)
• Binary Search: O(log n)
• Bubble Sort: O(n²)
Time Complexity Examples
• Constant Time – O(1):
• Example: Accessing an array element.
• Logarithmic Time – O(log n):
• Example: Binary search.
• Linear Time – O(n):
• Example: Traversing an array.
• Quadratic Time – O(n²):
• Example: Nested loops in Bubble Sort.
• Exponential Time – O(2ⁿ):
• Example: Recursive Fibonacci calculation.
For (…
 for (…
Best, Worst, and Average Cases
• Best-case scenario: Minimum time required.
• Worst-case scenario: Maximum time required.
• Average-case scenario: Expected time required.
• Example: QuickSort
• Best-case: O(n log n) (Balanced partitions)
• Worst-case: O(n²) (Unbalanced partitions)
• Average-case: O(n log n)
Space Complexity
• Memory required by an algorithm.
• Consists of:
• Fixed part: Constants, variables.
• Variable part: Dynamic memory allocation.
• Examples:
• Iterative algorithms: O(1) space.
• Recursive algorithms: O(n) (due to stack memory).
Trade-offs in Algorithm Selection
• Time vs Space Trade-off:
• Faster algorithms may use more memory.
• Space-efficient algorithms may be slower.
• Example:
• Merge Sort (O(n log n) time, O(n) space)
• QuickSort (O(n log n) time, O(log n) space)
Case Study – Sorting Algorithms
Algorithm Best Case Worst Case Average
Case
Space
Complexity
Bubble Sort O(n) O(n²) O(n²) O(1)
Merge Sort O(n log n) O(n log n) O(n log n) O(n)
Quick Sort O(n log n) O(n²) O(n log n) O(log n)
Practical Applications of Complexity
Analysis
• Data Processing: Optimizing large-scale computations.
• Machine Learning: Efficient model training.
• Cryptography: Secure encryption algorithms.
• Networking: Packet routing and load balancing.
• Database Systems: Query optimization.
Summary
• Algorithms: Step-by-step problem-solving procedures.
• Complexity Analysis: Helps evaluate efficiency.
• Asymptotic Notations: Big-O, Big-Theta, Big-Omega.
• Time vs Space Trade-offs: Finding the right balance.
• Applications: Real-world impact in computing and AI.
Mathematical Foundations
in Algorithms
Understanding the Mathematical Principles Behind Algorithm Design
Importance of Mathematical Foundations
• Why study mathematics in algorithms?
• Analyze algorithm efficiency (time/space complexity).
• Prove correctness and performance guarantees.
• Model problems using mathematical structures.
• Examples: Big-O notation, recurrence relations, probability in
randomized algorithms.
Growth of Functions
• Definitions:
• Big-O: Upper bound (worst case).
• Big-Ω: Lower bound (best case).
• Big-Θ: Tight bound (average case).
• Examples:
• Compare 𝑛
2
, 𝑛𝑙𝑜𝑔𝑛, 𝑎𝑛𝑑 2
𝑛
growth rates.
• Plot graphs to visualize asymptotic behavior.
Floor and
Ceiling
Fact about
Floor and
Ceiling
Powers of 2
Unsigned
binary
integers
Logs and
exponents
Properties of
logs
Arithmetic
Series
Algorithm
Analysis
Asymptotic
Behavior
Basic time
bounds
Analysis
Example of Analysis or Recursion Used Badly
Fibonacci Analysis
Iterative Algorithm for FN
Recursion
Summary
Divide and Conquer
Algorithm Design
An in-depth exploration of concepts, techniques,
and examples
Agenda
• Introduction to Divide and Conquer
• Key Principles of Divide and Conquer
• Recursion & Base Case
• Algorithm Design Process
• Case Studies & Examples
• Merge Sort
• Quick Sort
• Binary Search
• Time Complexity Analysis
• Applications and Use Cases
• Summary & Q&A
What is Divide
and Conquer?
• Definition and explanation:
• Break the problem into smaller sub-problems.
• Solve them recursively.
• Combine the solutions to solve the main
problem.
• Key steps:
• Divide
• Conquer
• Combine
Divide and Conquer -
Key Principles
• Recursion
• Base Case
• Combining Results
• Efficiency
Recursion &
Base Case
• Recursion: Breaking the problem down
into smaller sub-problems.
• Base Case: When to stop recursion
(problem size is small enough).
Divide and
Conquer -
Algorithm
Design Process
• Identify problems that can be recursively solved.
• Divide into smaller sub-problems.
• Solve each sub-problem.
• Combine results to solve the main problem.
Example 1:
Merge Sort
• Problem: Sorting an array. • Divide: Split the array in half. • Conquer: Recursively sort each half. • Combine: Merge the sorted halves.
Merge Sort
–
Detailed Steps
• Divide: Split the array. • Conquer: Sort each half recursively. • Combine: Merge the sorted halves.
Merge Sort Code Example
def merge_sort(arr):
 if len(arr) <= 1:
 return arr
 mid = len(arr) // 2
 left = merge_sort(arr[:mid])
 right = merge_sort(arr[mid:])
 return merge(left, right)
def merge(left, right):
 result = []
 while left and right:
 if left[0] < right[0]:
 result.append(left.pop(0))
 else:
 result.append(right.pop(0))
 result.extend(left or right)
 return result
Time
Complexity
of Merge
Sort
• Best, Worst, and Average Case: O(n log n)
• Reason: Divide step takes O(log n) and merge
step takes O(n).
Example 2:
Quick Sort
• Problem: Sorting an array.
• Divide: Choose a pivot, partition into two subarrays.
• Conquer: Recursively sort both sub-arrays.
• Combine: Sorted array is the result of the
recursion.
Quick Sort –
Detailed
Steps
• Divide: Choose a pivot, partition the array.
• Conquer: Recursively sort the sub-arrays.
• Combine: No combining step.
Quick Sort Code Example
def quick_sort(arr):
 if len(arr) <= 1:
 return arr
 pivot = arr[len(arr) // 2]
 left = [x for x in arr if x < pivot]
 middle = [x for x in arr if x == pivot]
 right = [x for x in arr if x > pivot]
 return quick_sort(left) + middle + quick_sort(right)
Time
Complexity
of Quick
Sort
• Best and Average Case: O(n log n)
• Worst Case: O(n²), occurs when the pivot choice
is poor.
Example 3:
Binary
Search
• Problem: Searching for a target value in a sorted
array.
• Divide: Find the middle element.
• Conquer: Search left or right based on
comparison.
• Combine: No combining step; result is found or
not.
Binary
Search
–
Detailed
Steps
• Divide: Find the middle element. • Conquer: Search in left or right based on
comparison.
• Combine: Return the result.
Binary Search Code Example
def binary_search(arr, target):
 low, high = 0, len(arr) - 1
 while low <= high:
 mid = (low + high) // 2
 if arr[mid] == target:
 return mid
 elif arr[mid] < target:
 low = mid + 1
 else:
 high = mid - 1
 return -1
Time
Complexity
of Binary
Search
• Best Case: O(1)
• Worst Case: O(log n)
Time Complexity Analysis - Divide and
Conquer
• General Recurrence:
• Example: Merge Sort:
• a: Number of subproblems
• n/b: Size of each subproblem
• F(n): Cost of dividing and combining
Master Theorem Cases
• Case 1:
• Case 2:
• Case 3:
Advantages
of Divide
and
Conquer
• Efficient for large problems
• Naturally suited for parallel processing
• Simplifies complex problems.
Disadvantages
of Divide and
Conquer
• Overhead of recursion
• Not always optimal for samll problems
• May require additional memory for combining
results.
Applications
of Divide and
Conquer
• Sorting Algorithms (Merge Sort, Quick Sort)
• Searching (Binary Search)
• Matrix Multiplication (Strassen's Algorithm)
• Closest Pair of Points
Other divide
and conquer
problems
Summary
• Divide and conquer breaks problems into
smaller sub-problems.
• Key algorithms: Merge Sort, Quick Sort, Binary
Search.
• Time complexity analysis helps optimize
algorithms.
DYNAMIC PROGRAMMING (DP) - A DETAILED
EXPLORATION
UNDERSTANDING THE PRINCIPLES, TECHNIQUES, AND APPLICATIONS OF DP
AGENDA
 Introduction to Dynamic Programming (DP)
 Key Concepts and Terminology
 Problem Solving Using DP
 Types of DP Problems
 Optimal Substructure
 Overlapping Subproblems
 Approaches to DP
 Top-Down (Memoization)
 Bottom-Up (Tabulation)
 Classic DP Problems
 Fibonacci Sequence
 Knapsack Problem
 Longest Common Subsequence
 Time Complexity of DP Solutions
 Applications of DP
 Summary and Q&A
WHAT IS DYNAMIC PROGRAMMING (DP)?
 Definition: DP is a method for solving complex problems by breaking them down into simpler
subproblems and solving each subproblem only once, storing its result.
 Key Idea: Overcome the inefficiency of recursive solutions by avoiding recomputation of overlapping
subproblems.
 Goal: Solve problems efficiently by storing intermediate results (memoization) or building up the solution
iteratively (tabulation).
KEY TERMINOLOGY IN DP
 Subproblem: A smaller, simpler version of the original problem.
 Optimal Substructure: The optimal solution to the problem can be constructed from optimal solutions to its
subproblems.
 Overlapping Subproblems: The same subproblems are solved multiple times in different parts of the recursive
tree.
THE DP APPROACH
 Characterize the structure of an optimal solution.
 Define the value of an optimal solution recursively (recurrence relation).
 Compute the value of an optimal solution (usually in a bottom-up manner).
 Construct the optimal solution (if required).
KEY DP PROPERTIES
 Optimal Substructure: A problem has optimal substructure if its optimal solution can be constructed efficiently
from the optimal solutions of its subproblems.
 Overlapping Subproblems: If the problem involves solving the same subproblems multiple times, DP helps by
storing the results to avoid recomputation.
TOP-DOWN APPROACH (MEMOIZATION)
 Top-Down DP (Memoization): Solve the problem using recursion, but store the results of subproblems in a cache
(often a table) to avoid recalculating them.
 Steps:
 Recursively solve the problem.
 Store the result of each subproblem.
 If the same subproblem is encountered again, use the stored result.
BOTTOM-UP APPROACH (TABULATION)
 Bottom-Up DP (Tabulation): Solve the problem iteratively, starting with the smallest subproblems and building up
to the final solution.
 Steps:
 Start solving the smallest subproblems.
 Use previously solved subproblems to build the solution.
 Tabulate the results in a table.
WHEN TO USE TOP-DOWN VS. BOTTOM-UP
 Top-Down (Memoization):
 Easier to implement and intuitive.
 Suitable when the problem has a complex recursive structure.
 May involve more memory (due to recursion stack).
 Bottom-Up (Tabulation):
 More efficient in terms of time and space.
 Avoids recursion overhead.
 Suitable when the problem has a simple iterative structure.
EXAMPLE 1 - FIBONACCI SEQUENCE
 Problem: Compute the nth Fibonacci number.
 Recurrence: 𝐹(𝑛)=𝐹(𝑛−1)+𝐹(𝑛−2)
 Base Case: F(0)=0,F(1)=1
FIBONACCI -TOP-DOWN (MEMOIZATION)
def fibonacci(n, memo={}):
 if n in memo:
 return memo[n]
 if n == 0: return 0
 if n == 1: return 1
 memo[n] = fibonacci(n-1, memo) + fibonacci(n-2, memo)
 return memo[n]
Explanation: Use a dictionary (memo)
to store the computed Fibonacci
values, avoiding redundant
calculations.
FIBONACCI - BOTTOM-UP (TABULATION)
 Explanation: Use an array to store
the Fibonacci values iteratively,
avoiding recursion.
def fibonacci(n):
 dp = [0] * (n+1)
 dp[1] = 1
 for i in range(2, n+1):
 dp[i] = dp[i-1] + dp[i-2]
 return dp[n]
TIME COMPLEXITY OF FIBONACCI
 Top-Down (Memoization): O(n) — Each subproblem is solved only once.
 Bottom-Up (Tabulation): O(n) — Iterative approach without recursion.
EXAMPLE 2 - 0/1 KNAPSACK PROBLEM
• Problem: Given a set of items, each with a weight and a value, determine the maximum value that can be
obtained with a knapsack of capacity W.
• Recurrence:
DP[i][w]=max(DP[i−1][w],DP[i−1][w−weight[i]]+value[i])
• If item i is not included, the value remains DP[i−1][w].
• If item i is included, the value becomes DP[i−1][w−weight[i]]+value[i]
KNAPSACK -TOP-DOWN (MEMOIZATION)
def knapsack(capacity, weights, values, n, memo={}):
 if n == 0 or capacity == 0:
 return 0
 if (n, capacity) in memo:
 return memo[(n, capacity)]

 if weights[n-1] > capacity:
 memo[(n, capacity)] = knapsack(capacity, weights, values,
n-1, memo)
 else:
 memo[(n, capacity)] = max(
 knapsack(capacity, weights, values, n-1, memo),
 values[n-1] + knapsack(capacity - weights[n-1], weights,
values, n-1, memo)
 )
 return memo[(n, capacity)]
O(nW)
KNAPSACK - BOTTOM-UP (TABULATION)
def knapsack(capacity, weights, values, n):
 dp = [[0] * (capacity + 1) for _ in range(n + 1)]
 for i in range(1, n + 1):
 for w in range(1, capacity + 1):
 if weights[i-1] <= w:
 dp[i][w] = max(dp[i-1][w], values[i-1] + dp[i-1][w -
weights[i-1]])
 else:
 dp[i][w] = dp[i-1][w]
 return dp[n][capacity]
O(nW)
EXAMPLE 3 - LONGEST COMMON SUBSEQUENCE (LCS)
 Problem: Given two strings, find the length of the longest subsequence common to both strings.
 Recurrence:
LCS - BOTTOM-UP (TABULATION)
def lcs(str1, str2):
 m, n = len(str1), len(str2)
 dp = [[0] * (n + 1) for _ in range(m + 1)]

 for i in range(1, m + 1):
 for j in range(1, n + 1):
 if str1[i-1] == str2[j-1]:
 dp[i][j] = dp[i-1][j-1] + 1
 else:
 dp[i][j] = max(dp[i-1][j], dp[i][j-1])
 return dp[m][n]
O(m * n) — m and n are the lengths of the two strings.
TIME COMPLEXITY OF DP SOLUTIONS
 Top-Down (Memoization): O(n * m) for most problems (n is the number of subproblems, m is the maximum size
of the subproblem).
 Bottom-Up (Tabulation): O(n * m) — space may be optimized from O(n * m) to O(m).
APPLICATIONS OF DP
 Optimization Problems: Knapsack, Shortest Path, Resource Allocation.
 String Matching and Parsing: LCS, Edit Distance, Longest Palindromic Subsequence.
 Machine Learning: Hidden Markov Models (HMM), Sequence Prediction.
 Bioinformatics: Sequence Alignment, Protein Folding.
COIN CHANGE
 Ne do të shkruajmë një funksion, Cmin, që ju tregon minimumin e monedhave që ju duhet të jepni si kusur.
 Për shembull me
 Cmin (13) = 3
 Nëse përpiqemi të llogarisim Cmin(n) dhe kemi informacionin për Cmin(n −1) Cmin(n −2) Cmin(n −5) dhe
Cmin(n −10) atëherë duhet të jeni në gjendje të përpunojë Cmin(n).
COIN CHANGE
 Nese Cmin(1) = Cmin(2) = Cmin(5) = Cmin(10) = 1 dhe Cmin(0) = 0.
 Pra, nëse duam të llogarisim Cmin(13) ne shikojmë në
Cmin(13 − 1), Cmin (13 − 2), Cmin (13 − 5) dhe Cmin (13 − 10)
Cmin(n) = min{1 + Cmin(n − k) | n − k ≥ 0, k ∈ {1, 2, 5, 10}}
COIN CHANGE NE PYTHON
VERTETONI QE FORMULAVLENE
 Të vërtetosh korrektësinë e ekuacionit rekurziv do të thotë që jeduke treguar
se zgjidhja e Cmin(n) mund të llogaritet vetëm duke parë nënproblemet.
 Ne po përpiqemi të llogarisim numrin optimal të monedhave, prandaj quhet
vetia optimale e nënstrukturës.
Cmin(n) = min{1 + Cmin(n − k) | n − k ≥ 0, k ∈ {1, 2, 5, 10}}
COIN CHANGE BOTTOM UP
Optimal matrix multiplication
a
b
b
c
=
A = a x b matrix
B = b x c matrix
How many operations to compute AB ?
Optimal matrix multiplication
a
b
b
c
=
Optimal matrix multiplication
a
b
b
c
=
each entry of A * B takes (b) time
need to compute ac entries  (abc) time
total
Optimal matrix multiplication
N x N
matrix
N x N
matrix
A B
Compute AB, time = ?
Optimal matrix multiplication
N x N
matrix
N x N
matrix
A B
Compute AB, time = (N3
)
Optimal matrix multiplication
N x N
matrix
N x N
matrix
A B
Compute AB, time = (N3
)
Compute ABC, time = ?
CN x 1 matrix
Optimal matrix multiplication
N x N
matrix
N x N
matrix
A B
Compute D=BC, time = ?
Compute AD, time = ?
Compute ABC, time = ?
CN x 1 matrix
Optimal matrix multiplication
N x N
matrix
N x N
matrix
A B
Compute D=BC, time = (N2
)
Compute AD, time = ?
Compute ABC, time = ?
CN x 1 matrix
Optimal matrix multiplication
N x N
matrix
A
Compute D=BC, time = (N2
)Compute AD, time = ?
Compute ABC, time = ?
DN x 1 matrix
Optimal matrix multiplication
N x N
matrix
A
Compute D=BC, time = (N2
)
Compute AD, time = (N2
)
Compute ABC, time = (N2
) DN x 1 matrix
Optimal matrix multiplication
N x N
matrix
N x N
matrix
A B
(AB)C = ABC = A(BC)
CN x 1 matrix
The order of evaluation
does not change the result
can change the amount of work needed
Optimal matrix multiplication
a1
,a2
,a3
,….,an
n-1 matrices of sizes
a1 x a2 B1
a2 x a3 B
2
a3 x a4 B3
….
an-1 x an Bn-1
What order should we multiply them in?
Optimal matrix multiplication
B1 B2 B3 B4 … Bn-1
B1
(B2 B3 B4 … Bn-1 )
(B1 B2
) (B3 B4 … Bn-1
)
(B1 B2 B3
) (B4 … Bn-1
)
…
(B1 B2 B3 B4 …) Bn-1
Optimal matrix multiplication
B1 B2 B3 B4 … Bn-1
K[i,j] = the minimal number of operations
needed to multiply Bi … Bj
Bi
(Bi+1 Bi+2 … Bj
)
(Bi Bi+1) (Bi+2 … Bj
)
(Bi Bi+1 Bi+2) ( … Bj
)
…
(B1 B2 B3 …) Bj
K[i,i] + K[i+1,j] + aiai+1aj+1
K[i,i+1] + K[i+2,j] + aiai+2aj+1
K[i,i+2] + K[i+3,j] + aiai+3aj+1
K[i,j-1] + K[j,j] + aiajaj+1
Optimal matrix multiplication
B1 B2 B3 B4 … Bn-1
K[i,j] = the minimal number of operations
needed to multiply Bi … Bj
K[i,j] = min K[i,w] + K[w+1,j] + ai aw+1 aj
iw< j
K[i,i]=0
Travelling Salesman Problem
INPUT:
N cities, NxN symmetric matrix D,
D(i,j) = distance between city i and j
OUTPUT:
the shortest tour visiting all the cities
Travelling Salesman Problem
Algorithm 1 – try all possibilities
for each permutation  of {1,...,n}
visit the cities in the order ,
compute distance travelled,
pick the best solution
running time = ?
Travelling Salesman Problem
Algorithm 1 – try all possibilities
for each permutation  of {1,...,n}
visit the cities in the order ,
compute distance travelled,
pick the best solution
running time  n! is (n+1)! = O(n!) ?
Travelling Salesman Problem
for each subset S of the cities with
|S|  2 and each u,v S
K[S,u,v] the length of the shortest path that
* starts at u
* ends at v
* visits all cities in S
How large is K ?
Travelling Salesman Problem
for each subset S of the cities with
|S|  2 and each u,v S
K[S,u,v] the length of the shortest path that
* starts at u
* ends at v
* visits all cities in S
How large is K ?
 2
n n2
Travelling Salesman Problem
K[S,u,v]
some vertex w  S – {u,v}
must be visited first
d(u,w) = we get to w
K[S-u,w,v] = we need to get from w to v
and visit all vertices in S-u
Travelling Salesman Problem
K[S,u,v] the length of the shortest path that
* starts at u
* ends at v
* visits all cities in S
if S={u,v} then K[S,u,v]=d(u,v)
if |S|>2 then
K[S,u,v] = min K[S-u,w,v] + d(u,w)
wS-{uv}
Travelling Salesman Problem
if S={u,v} then K[S,u,v]=d(u,v)
if |S|>2 then
K[S,u,v] = min K[S-u,w,v] + d(u,w)
wS-{u,v}
Running time = ?
K  2
n n2
Travelling Salesman Problem
if S={u,v} then K[S,u,v]=d(u,v)
if |S|>2 then
K[S,u,v] = min K[S-u,w,v] + d(u,w)
wS-{u,v}
Running time = O(n3 2
n)
K  2
n n2
Travelling Salesman Problem
dynamic programming = O(n3 2
n)
brute force = O(n!)
SUMMARY
 DP is a powerful method for solving optimization problems by breaking them down into simpler subproblems.
 It uses the concept of overlapping subproblems and optimal substructure.
 Common approaches: Top-Down (Memoization) and Bottom-Up (Tabulation).
 Classic problems include Fibonacci, Knapsack, and Longest Common Subsequence.
 DP can be applied to a wide range of problems in computer science and real-world applications.

Greedy Algorithms
Understanding the Principles, Techniques, and Applications of
Greedy Algorithms
Agenda
• Introduction to Greedy Algorithms
• Characteristics of Greedy Algorithms
• Greedy vs. Dynamic Programming
• Common Greedy Algorithm Patterns
• Examples of Greedy Algorithms
• Activity Selection Problem
• Huffman Coding
• Kruskal’s Algorithm
• Prim’s Algorithm
• Dijkstra’s Algorithm
• Fractional Knapsack
• Time Complexity of Greedy Algorithms
• Applications of Greedy Algorithms
• Summary and Q&A
What is a Greedy
Algorithm?
• Definition: A greedy algorithm builds
up a solution step by step, always
choosing the option that looks best at
the moment.
• Key Idea: Make the locally optimal
choice at each step, hoping that these
choices lead to a globally optimal
solution.
• Examples: Shortest path algorithms,
Huffman encoding, spanning trees,
etc.
Characteristics of
Greedy Algorithms
• Greedy Choice Property: An optimal
solution can be obtained by choosing a local
optimum at each step.
• Optimal Substructure: A problem has an
optimal substructure if an optimal solution
to the problem contains optimal solutions to
its subproblems.
• No Backtracking: Unlike Dynamic
Programming or Backtracking, Greedy
Algorithms do not reconsider choices.
• Efficient: They typically work in O(n log n) or
O(n) time complexity.
When to Use a Greedy
Algorithm?
• Use Greedy Algorithms if:
• The problem exhibits optimal substructure and
greedy choice property.
• You can sort or prioritize elements based on a
property (e.g., weight, value, or distance).
• A locally optimal choice leads to a globally
optimal solution.
• Do NOT use if:
• The problem requires backtracking or dynamic
programming for optimal results.
• Future choices depend on previous ones (e.g.,
0/1 Knapsack problem).
Greedy vs. Dynamic Programming
Feature Greedy Algorithm Dynamic Programming
Decision Making Makes decisions based on the
current best option
Makes decisions considering
future consequences
Backtracking No backtracking Uses memoization or tabulation
Subproblems Solves only necessary
subproblems Solves all subproblems
Example Prim’s Algorithm, Huffman
Coding Knapsack, Floyd-Warshall
Example 1 - Activity Selection
Problem
• Problem: Given n activities with start and
finish times, select the maximum number of
activities that can be performed by a single
person.
• Greedy Approach:
• Sort activities by end time.
• Select the first activity and iterate through
others, selecting only those that do not
overlap.
Activity Selection Algorithm (Code)
def activity_selection(activities):
 activities.sort(key=lambda x: x[1]) # Sort by end time
 selected = [activities[0]]

 for i in range(1, len(activities)):
 if activities[i][0] >= selected[-1][1]: # No overlap
condition
 selected.append(activities[i])

 return selected Time Complexity: O(nlogn)O(n \log n)O(nlogn) (due to
sorting)
Example 2 - Huffman Coding
• Problem: Given characters and their
frequencies, construct an optimal binary tree
(prefix-free) to minimize encoding length.
• Steps:
• Build a priority queue of characters
sorted by frequency.
• Extract two smallest elements, merge
them, and reinsert into the queue.
• Repeat until only one tree remains.
Huffman Coding Algorithm (Code)
import heapq
class Node:
 def __init__(self, char, freq):
 self.char = char
 self.freq = freq
 self.left = None
 self.right = None

 def __lt__(self, other):
 return self.freq < other.freq
def huffman_coding(freq_table):
 heap = [Node(char, freq) for char, freq in freq_table.items()]
 heapq.heapify(heap)

 while len(heap) > 1:
 left = heapq.heappop(heap)
 right = heapq.heappop(heap)
 merged = Node(None, left.freq + right.freq)
 merged.left, merged.right = left, right
 heapq.heappush(heap, merged)

 return heap[0] # Root of Huffman Tree
Time Complexity: O(nlogn)O(n \log n)O(nlogn)
Example 3 - Minimum Spanning Tree
(Kruskal's Algorithm)
• Problem: Find the Minimum Spanning Tree (MST) for a weighted
graph.
• Steps:
• Sort edges by weight.
• Use a disjoint-set (union-find) structure to check cycles.
• Add edges to the MST while ensuring no cycles.
Detecting Cycles
• If the edge to be added (u,v) is such that
vertices u and v belong to the same tree,
then by adding (u,v) you would form a cycle
• Therefore to check, Find(u) and Find(v).
If they are the same discard (u,v)
• If they are different
Union(Find(u),Find(v))
Example
1
2 3 4
6 5
10
1
5
8 3
1 1 6
2
4
1
1
2 3 4
6 5
10
1
5
8 3
1 1 6
2
4
1
Initialization
1
2 3 4
6 5
Initially, Forest of 6 trees
F= {{1},{2},{3},{4},{5},{6}}
Edges in a heap (not
shown)
Step 1
1
2 3 4
6 5
Select edge with lowest
cost (2,5)
Find(2) = 2, Find (5) = 5
Union(2,5)
F= {{1},{2,5},{3},{4},{6}}
1 edge accepted 1
Step 2
1
2 3 4
6 5
Select edge with lowest
cost (2,6)
Find(2) = 2, Find (6) = 6
Union(2,6)
F= {{1},{2,5,6},{3},{4}}
2 edges accepted 1
1
Step 3
1
2 3 4
6 5
Select edge with lowest
cost (1,3)
Find(1) = 1, Find (3) = 3
Union(1,3)
F= {{1,3},{2,5,6},{4}}
3 edges accepted 1
1
1
Step 4
1
2 3 4
6 5
Select edge with lowest
cost (5,6)
Find(5) = 2, Find (6) = 2
Do nothing
F= {{1,3},{2,5,6},{4}}
3 edges accepted 1
1
1
Step 5 1 2 3 4 6 5
Select edge with lowest
cost (3,4)
Find(3) = 1, Find (4) = 4
Union(1,4)
F= {{1,3,4},{2,5,6}}
4 edges accepted
1
1
1
3
Step 6 1 2 3 4 6 5
Select edge with lowest
cost (4,5)
Find(4) = 1, Find (5) = 2
Union(1,2)
F= {{1,3,4,2,5,6}}
5 edges accepted : end
Total cost = 10
Although there is a unique
spanning tree in this
example, this is not
generally the case
1
1
1
34
Kruskal’s Algorithm
Analysis
• Initialize forest O(n)
• Initialize heap O(m), m = |E|
• Loop performed m times
• In the loop one DeleteMin O(log m)
• Two Find, each O(log n)
• One Union (at most) O(1)
• So worst case O(m log m) = O(m log n)
Time Complexity Summary
• Recall that m = |E| = O(V2
) = O(n2 )
• Prim’s runs in O((n+m) log n)
• Kruskal runs in O(m log m) = O(m log n)
• In practice, Kruskal has a tendency to run faster since graphs
might not be dense and not all edges need to be looked at in the
Deletemin operations
Kruskal's Algorithm (Code)
def kruskal(edges, n):
 edges.sort(key=lambda x: x[2]) # Sort by weight
 parent = list(range(n))

 def find(v):
 if parent[v] != v:
 parent[v] = find(parent[v])
 return parent[v]

 mst = []
 for u, v, w in edges:
 root_u, root_v = find(u), find(v)
 if root_u != root_v:
 mst.append((u, v, w))
 parent[root_u] = root_v # Union operation

 return mst
Time Complexity: O(ElogE)O(E \log E)O(ElogE)
Example 4 - Dijkstra’s Algorithm
• Problem: Find the shortest path from a source node to all other nodes in a graph.
• Greedy Approach:
• Use a priority queue (min-heap).
• Pick the nearest unvisited node and update its neighbors.
Step 1- Create a table
• Step 2- Update table with
Edges reachable from
Source
• Step 3. Repeat for each
edge starting from the one
with shortest distance.
Greedy Algorithm
Strengths
• Simple and intuitive.
• Works well when greedy choice property
holds.
• Faster than Dynamic Programming when
applicable.
Greedy Algorithm
Weaknesses
• May not provide an optimal solution in
all cases.
• Cannot handle backtracking or future
consequences.
• Often requires problem-specific
proofs of correctness.
Applications of
Greedy Algorithms
• Networking: Routing, Load Balancing.
• Data Compression: Huffman Coding.
• Graph Theory: Shortest Paths, MST.
• Scheduling: Activity Selection, Task
Scheduling.
Summary
• Greedy algorithms make locally
optimal choices at each step.
• Work well for problems with greedy
choice property and optimal
substructure.
• Used in graph algorithms,
scheduling, and optimization
problems.
Graph Algorithms - Basics
Understanding Graphs, Their Representations, and Fundamental
Algorithms
Agenda
• Introduction to Graphs
• Graph Representations
• Graph Traversal Algorithms
• Breadth-First Search (BFS)
• Depth-First Search (DFS)
• Connected Components
• Basic Shortest Path Algorithms
• Applications of Graph Algorithms
• Summary and Q&A
What is a Graph?
• A graph is a data structure consisting of:
• Vertices (Nodes): Represent entities.
• Edges (Connections): Represent relationships.
• Graphs are used to model real-world relationships such as social
networks, road maps, and communication networks.
• Types of Graphs:
• Directed vs. Undirected
• Weighted vs. Unweighted
• Cyclic vs. Acyclic
Graph Terminology
• Degree: Number of edges connected to a node.
• Path: A sequence of edges connecting two vertices.
• Cycle: A path that starts and ends at the same vertex.
• Connected Graph: A graph where every pair of vertices is
connected.
• Tree: A connected acyclic graph.
Graph Representations
1. Adjacency Matrix
• A 2D array where matrix[i][j] = 1 if there is an edge between i and j.
• Space Complexity: O(V^2)
• Efficient for Dense Graphs
2. Adjacency List
• An array of lists, where list[i] contains all neighbors of vertex i.
• Space Complexity: O(V + E)
• Efficient for Sparse Graphs
Adjacency Matrix vs. Adjacency List
Representation Space Complexity Lookup Time Best for
Adjacency Matrix O(V^2) O(1) Dense Graphs
Adjacency List O(V + E) O(degree) Sparse Graphs
Graph Traversal Algorithms
Graph traversal is used to visit all nodes in a graph systematically.
• Breadth-First Search (BFS): Explores level by level.
• Depth-First Search (DFS): Explores as deep as possible before
backtracking.
Breadth-First Search (BFS)
• Uses a Queue (FIFO)
• Explores neighbors before moving deeper.
• Useful for finding shortest path in an unweighted graph.
• Time Complexity: O(V + E)
BFS Algorithm:
1.Push the starting node into the queue.
2.Mark it as visited.
3.While the queue is not empty:
1. Dequeue a node.
2. Process it.
3. Enqueue all unvisited neighbors.
BFS Example
(Graph Diagram)
Depth-First Search (DFS)
• Uses a Stack (or Recursion)
• Explores deep before backtracking.
• Useful for cycle detection, topological sorting, and connected
components.
• Time Complexity: O(V + E)
DFS Algorithm:
1.Start at a node.
2.Mark it as visited.
3.Recursively visit all its neighbors.
DFS Example (Graph Diagram)
Connected Components
• A connected component is a subset of a graph where all nodes
are connected to each other.
• Finding Components using DFS/BFS:
• Traverse each unvisited node and mark all reachable nodes as part of the
same component.
Basic Shortest Path Algorithms
• Dijkstra’s Algorithm: Works on graphs with non-negative weights.
• Bellman-Ford Algorithm: Handles graphs with negative weights.
Applications of Graph Algorithms
• Social Networks: Friend suggestions, community detection.
• Navigation Systems: Google Maps (Dijkstra’s Algorithm).
• Network Routing: Internet packet transmission.
• AI & Machine Learning: Graph-based recommendation systems.
Summary
• Graphs model real-world problems.
• Different ways to represent graphs (Adjacency List vs. Matrix).
• BFS and DFS are fundamental traversal methods.
• Connected components and shortest path algorithms have broad
applications.